{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRakA_kOy4iX",
        "outputId": "c1dad5d8-0aed-4e0d-cf7f-ba670a8a7517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    ╔════════════════════════════════════════════════════════════════╗\n",
            "    ║  MOVIELENS HYBRID RECOMMENDER - GITHUB CODESPACES READY        ║\n",
            "    ╠════════════════════════════════════════════════════════════════╣\n",
            "    ║                                                                ║\n",
            "    ║  This script will:                                             ║\n",
            "    ║  1. Auto-download MovieLens Latest-Small dataset (100K)        ║\n",
            "    ║  2. Load and prepare data with perfect joins (NO mismatch!)    ║\n",
            "    ║  3. Train hybrid collaborative + content-based model           ║\n",
            "    ║  4. Evaluate performance (expected RMSE: 0.85-0.88)            ║\n",
            "    ║  5. Generate sample recommendations                            ║\n",
            "    ║  6. Create visualization plots                                 ║\n",
            "    ║                                                                ║\n",
            "    ║  No manual setup required - just run!                          ║\n",
            "    ║                                                                ║\n",
            "    ╚════════════════════════════════════════════════════════════════╝\n",
            "    \n",
            "\n",
            "██████████████████████████████████████████████████████████████████████\n",
            "   COMPLETE MOVIELENS HYBRID RECOMMENDER SYSTEM\n",
            "   GitHub Codespaces Ready - Auto Setup\n",
            "██████████████████████████████████████████████████████████████████████\n",
            "✓ Dataset already exists at ./ml-latest-small\n",
            "\n",
            "======================================================================\n",
            "LOADING MOVIELENS DATA\n",
            "======================================================================\n",
            "\n",
            "[1/3] Loading ratings.csv...\n",
            "✓ Loaded 100,836 ratings\n",
            "  → Users: 610\n",
            "  → Movies: 9,724\n",
            "  → Rating range: 0.5 - 5.0\n",
            "\n",
            "[2/3] Loading movies.csv...\n",
            "✓ Loaded 9,742 movies\n",
            "\n",
            "[3/3] Loading tags.csv...\n",
            "✓ Loaded 3,683 tags\n",
            "  → Movies with tags: 1,572\n",
            "\n",
            "🔗 Merging datasets...\n",
            "  ✓ Ratings + Movies: 100.00% success (Expected: 100%)\n",
            "\n",
            "✓ Merged dataset: 100,836 ratings\n",
            "\n",
            "Filtering (min 5 ratings/user, 5 ratings/movie)...\n",
            "  → Removed sparse users/movies\n",
            "  → Final dataset: 90,274 ratings\n",
            "\n",
            "======================================================================\n",
            "DATASET SUMMARY\n",
            "======================================================================\n",
            "Total ratings:      90,274\n",
            "Unique users:       610\n",
            "Unique movies:      3,650\n",
            "Unique genres:      19\n",
            "Sparsity:           0.959455 (4.055% filled)\n",
            "Avg ratings/user:   148.0\n",
            "Avg ratings/movie:  24.7\n",
            "Rating distribution:\n",
            "  0.5:  1,110 (  1.2%)\n",
            "  1.0:  2,362 (  2.6%)\n",
            "  1.5:  1,389 (  1.5%)\n",
            "  2.0:  6,419 (  7.1%)\n",
            "  2.5:  4,702 (  5.2%)\n",
            "  3.0: 18,000 ( 19.9%)\n",
            "  3.5: 11,527 ( 12.8%)\n",
            "  4.0: 24,549 ( 27.2%)\n",
            "  4.5:  7,765 (  8.6%)\n",
            "  5.0: 12,451 ( 13.8%)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "GENERATING VISUALIZATIONS\n",
            "======================================================================\n",
            "✓ Saved data statistics: data_statistics.png\n",
            "\n",
            "======================================================================\n",
            "TRAIN/VALIDATION/TEST SPLIT (60/20/20)\n",
            "======================================================================\n",
            "Train:       54,164 ratings ( 60.0%)\n",
            "Validation:  18,055 ratings ( 20.0%)\n",
            "Test:        18,055 ratings ( 20.0%)\n",
            "\n",
            "======================================================================\n",
            "INITIALIZING HYBRID MODEL\n",
            "======================================================================\n",
            "Hyperparameters:\n",
            "  • Latent factors: 20\n",
            "  • Max iterations: 60\n",
            "  • Regularization: 0.25\n",
            "  • Learning rate: 0.005\n",
            "  • Content weight: 15%\n",
            "  • CF weight: 85%\n",
            "\n",
            "--- Building Content Features ---\n",
            "✓ Unique genres: 19\n",
            "✓ Items with genre info: 3,645\n",
            "\n",
            "--- Initializing Model Parameters ---\n",
            "✓ User factors (P): (610, 20)\n",
            "✓ Item factors (Q): (3650, 20)\n",
            "✓ Parameters: 89,460\n",
            "Building user profiles...\n",
            "✓ Built profiles for 610 users\n",
            "\n",
            "======================================================================\n",
            "TRAINING HYBRID MODEL\n",
            "======================================================================\n",
            "Architecture: 610 users × 3,650 items × 20 factors\n",
            "Training samples: 54,164\n",
            "Validation samples: 18,055\n",
            "Hybrid weights: CF=85%, Content=15%\n",
            "Regularization: λ=0.25\n",
            "Learning rate: α=0.005\n",
            "======================================================================\n",
            "Epoch   1/60 | Train: 0.9151 | Val: 0.9330 | Gap: +0.0179 ✓\n",
            "Epoch   5/60 | Train: 0.8584 | Val: 0.8903 | Gap: +0.0319 ✓\n",
            "Epoch  10/60 | Train: 0.8372 | Val: 0.8768 | Gap: +0.0396 ✓\n",
            "Epoch  15/60 | Train: 0.8261 | Val: 0.8705 | Gap: +0.0444 ✓\n",
            "Epoch  20/60 | Train: 0.8191 | Val: 0.8668 | Gap: +0.0477 ✓\n",
            "Epoch  25/60 | Train: 0.8144 | Val: 0.8646 | Gap: +0.0502 ✓\n",
            "Epoch  30/60 | Train: 0.8110 | Val: 0.8630 | Gap: +0.0521 ✓\n",
            "Epoch  35/60 | Train: 0.8079 | Val: 0.8617 | Gap: +0.0538 ✗\n",
            "Epoch  40/60 | Train: 0.8060 | Val: 0.8613 | Gap: +0.0552 ✓\n",
            "Epoch  45/60 | Train: 0.8043 | Val: 0.8605 | Gap: +0.0562 ✗\n",
            "Epoch  50/60 | Train: 0.8030 | Val: 0.8605 | Gap: +0.0575 ✗\n",
            "Epoch  55/60 | Train: 0.8016 | Val: 0.8597 | Gap: +0.0581 ✓\n",
            "Epoch  60/60 | Train: 0.8007 | Val: 0.8594 | Gap: +0.0587 ✓\n",
            "\n",
            "✓ Training completed!\n",
            "✓ Best model from epoch 60 (Val RMSE: 0.8594)\n",
            "\n",
            "✓ Saved learning curves: learning_curves.png\n",
            "\n",
            "======================================================================\n",
            "FINAL EVALUATION ON TEST SET\n",
            "======================================================================\n",
            "Generating test predictions...\n",
            "Evaluating all sets...\n",
            "\n",
            "======================================================================\n",
            "FINAL RESULTS\n",
            "======================================================================\n",
            "Dataset            RMSE        MAE      Samples\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Train            0.8007     0.6177       54,164\n",
            "Validation       0.8594     0.6616       18,055\n",
            "Test             0.8575     0.6570       18,055\n",
            "======================================================================\n",
            "\n",
            "📊 PERFORMANCE ANALYSIS:\n",
            "\n",
            "  Train-Val Gap: +0.0587\n",
            "    ✅ Good generalization\n",
            "\n",
            "  Val-Test Difference: 0.0020\n",
            "    ✅ Excellent! Validation accurately predicts test performance\n",
            "✓ Saved prediction analysis: predictions_analysis.png\n",
            "\n",
            "======================================================================\n",
            "CREATING RECOMMENDATION INTERFACE\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "SAMPLE RECOMMENDATIONS\n",
            "======================================================================\n",
            "\n",
            "══════════════════════════════════════════════════════════════════════\n",
            "SAMPLE USER #1: ID=298 (842 ratings)\n",
            "══════════════════════════════════════════════════════════════════════\n",
            "\n",
            "📚 Rating History (Top 5):\n",
            "                                                                         title                           genres  rating primary_genre\n",
            "                                                         Godfather, The (1972)                      Crime|Drama     5.0         Crime\n",
            "                                                             Fight Club (1999)      Action|Crime|Drama|Thriller     5.0        Action\n",
            "                                                      Big Lebowski, The (1998)                     Comedy|Crime     5.0        Comedy\n",
            "Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)                 Action|Adventure     4.5        Action\n",
            "              Cowboy Bebop: The Movie (Cowboy Bebop: Tengoku no Tobira) (2001) Action|Animation|Sci-Fi|Thriller     4.5        Action\n",
            "\n",
            "🎯 Top 10 Recommendations:\n",
            " movieId                                                 title                      genres  predicted_rating  avg_rating\n",
            "    1178                                 Paths of Glory (1957)                   Drama|War              3.55        4.54\n",
            "    3266 Man Bites Dog (C'est arrivé près de chez vous) (1992) Comedy|Crime|Drama|Thriller              3.51        4.50\n",
            "  177593      Three Billboards Outside Ebbing, Missouri (2017)                 Crime|Drama              3.48        4.75\n",
            "    1041                                 Secrets & Lies (1996)                       Drama              3.45        4.59\n",
            "     306      Three Colors: Red (Trois couleurs: Rouge) (1994)                       Drama              3.45        4.22\n",
            "   93838                           The Raid: Redemption (2011)                Action|Crime              3.39        4.33\n",
            "    1241                         Dead Alive (Braindead) (1992)       Comedy|Fantasy|Horror              3.39        4.05\n",
            "    1233                          Boot, Das (Boat, The) (1981)            Action|Drama|War              3.38        4.21\n",
            "    3468                                   Hustler, The (1961)                       Drama              3.38        4.33\n",
            "  158966                              Captain Fantastic (2016)                       Drama              3.37        4.50\n",
            "\n",
            "══════════════════════════════════════════════════════════════════════\n",
            "SAMPLE USER #2: ID=325 (237 ratings)\n",
            "══════════════════════════════════════════════════════════════════════\n",
            "\n",
            "📚 Rating History (Top 5):\n",
            "              title                      genres  rating primary_genre\n",
            "        Safe (1995)                    Thriller     5.0      Thriller\n",
            " Taxi Driver (1976)        Crime|Drama|Thriller     5.0         Crime\n",
            "       Smoke (1995)                Comedy|Drama     5.0        Comedy\n",
            "Pulp Fiction (1994) Comedy|Crime|Drama|Thriller     5.0        Comedy\n",
            "     Ed Wood (1994)                Comedy|Drama     5.0        Comedy\n",
            "\n",
            "🎯 Top 10 Recommendations:\n",
            " movieId                                                 title                      genres  predicted_rating  avg_rating\n",
            "    1178                                 Paths of Glory (1957)                   Drama|War              4.45        4.54\n",
            "    3266 Man Bites Dog (C'est arrivé près de chez vous) (1992) Comedy|Crime|Drama|Thriller              4.37        4.50\n",
            "  177593      Three Billboards Outside Ebbing, Missouri (2017)                 Crime|Drama              4.35        4.75\n",
            "    1041                                 Secrets & Lies (1996)                       Drama              4.35        4.59\n",
            "     306      Three Colors: Red (Trois couleurs: Rouge) (1994)                       Drama              4.35        4.22\n",
            "  158966                              Captain Fantastic (2016)                       Drama              4.28        4.50\n",
            "    3451                   Guess Who's Coming to Dinner (1967)                       Drama              4.27        4.55\n",
            "    3468                                   Hustler, The (1961)                       Drama              4.25        4.33\n",
            "    1241                         Dead Alive (Braindead) (1992)       Comedy|Fantasy|Horror              4.24        4.05\n",
            "    1233                          Boot, Das (Boat, The) (1981)            Action|Drama|War              4.23        4.21\n",
            "\n",
            "══════════════════════════════════════════════════════════════════════\n",
            "SAMPLE USER #3: ID=3 (28 ratings)\n",
            "══════════════════════════════════════════════════════════════════════\n",
            "\n",
            "📚 Rating History (Top 5):\n",
            "                               title                           genres  rating primary_genre\n",
            "             Escape from L.A. (1996) Action|Adventure|Sci-Fi|Thriller     5.0        Action\n",
            "Road Warrior, The (Mad Max 2) (1981) Action|Adventure|Sci-Fi|Thriller     5.0        Action\n",
            "          Conan the Barbarian (1982)         Action|Adventure|Fantasy     4.5        Action\n",
            "                      Piranha (1978)                    Horror|Sci-Fi     4.5        Horror\n",
            "                   Thing, The (1982)    Action|Horror|Sci-Fi|Thriller     4.0        Action\n",
            "\n",
            "🎯 Top 10 Recommendations:\n",
            " movieId                       title                   genres  predicted_rating  avg_rating\n",
            "  142488            Spotlight (2015)                 Thriller              2.74        4.16\n",
            "    3727            Near Dark (1987)           Horror|Western              2.73        4.31\n",
            "  176371    Blade Runner 2049 (2017)                   Sci-Fi              2.67        3.81\n",
            "    1258         Shining, The (1980)                   Horror              2.67        4.08\n",
            "    4437             Suspiria (1977)                   Horror              2.65        3.94\n",
            "    1248        Touch of Evil (1958) Crime|Film-Noir|Thriller              2.65        4.26\n",
            "     904          Rear Window (1954)         Mystery|Thriller              2.64        4.26\n",
            "   93838 The Raid: Redemption (2011)             Action|Crime              2.64        4.33\n",
            "     246          Hoop Dreams (1994)              Documentary              2.63        4.29\n",
            "   27773              Old Boy (2003)         Mystery|Thriller              2.62        4.09\n",
            "\n",
            "██████████████████████████████████████████████████████████████████████\n",
            "✅ SYSTEM EXECUTION COMPLETED SUCCESSFULLY!\n",
            "██████████████████████████████████████████████████████████████████████\n",
            "\n",
            "📊 SYSTEM SUMMARY:\n",
            "  • Total ratings processed: 90,274\n",
            "  • Unique users: 610\n",
            "  • Unique movies: 3,650\n",
            "  • Genres available: 19\n",
            "  • Data sparsity: 0.959455\n",
            "\n",
            "🎯 MODEL PERFORMANCE:\n",
            "  • Test RMSE: 0.8575\n",
            "  • Test MAE: 0.6570\n",
            "  • Training epochs: 60\n",
            "  • Best epoch: 60\n",
            "\n",
            "🔧 HYBRID ARCHITECTURE:\n",
            "  • Collaborative filtering: 85%\n",
            "  • Content-based: 15%\n",
            "  • Latent factors: 20\n",
            "  • Total parameters: 89,460\n",
            "\n",
            "📁 OUTPUT FILES:\n",
            "  ✓ data_statistics.png - Dataset overview\n",
            "  ✓ learning_curves.png - Training progress\n",
            "  ✓ predictions_analysis.png - Prediction quality\n",
            "\n",
            "💡 USAGE EXAMPLES:\n",
            "  # Get recommendations for a user\n",
            "  recommendations = recommender.recommend(user_id=42, n=10)\n",
            "\n",
            "  # View user's rating history\n",
            "  history = recommender.get_user_history(user_id=42)\n",
            "\n",
            "  # Make a prediction\n",
            "  prediction = model.predict_hybrid(user_idx=5, item_idx=100)\n",
            "\n",
            "🚀 NEXT STEPS:\n",
            "  1. Experiment with hyperparameters (n_factors, reg_lambda)\n",
            "  2. Try different content features (tags, timestamps)\n",
            "  3. Implement user-user or item-item collaborative filtering\n",
            "  4. Add confidence intervals for predictions\n",
            "  5. Build a web interface with Flask/Streamlit\n",
            "\n",
            "██████████████████████████████████████████████████████████████████████\n",
            "\n",
            "======================================================================\n",
            "SUCCESS! Model and recommender ready to use.\n",
            "======================================================================\n",
            "\n",
            "💾 Objects available in memory:\n",
            "  • model: HybridMatrixFactorization instance\n",
            "  • recommender: MovieRecommender interface\n",
            "  • data: Complete dataset (pandas DataFrame)\n",
            "  • test_rmse: Final test performance\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "COMPLETE MOVIELENS HYBRID RECOMMENDER SYSTEM\n",
        "GitHub Codespaces Ready - Auto-downloads data\n",
        "Perfect for recommendation systems with GUARANTEED results\n",
        "\n",
        "Author: AI Assistant\n",
        "Dataset: MovieLens Latest-Small (100K ratings)\n",
        "Expected Performance: RMSE 0.85-0.88\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Set\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# ============================================================================\n",
        "# PART 1: AUTO-DOWNLOAD AND SETUP\n",
        "# ============================================================================\n",
        "\n",
        "class DatasetSetup:\n",
        "    \"\"\"Automatically downloads and prepares MovieLens dataset\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir: str = \"./ml-latest-small\"):\n",
        "        self.data_dir = data_dir\n",
        "        self.url = \"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n",
        "        self.zip_path = \"ml-latest-small.zip\"\n",
        "\n",
        "    def download_and_extract(self):\n",
        "        \"\"\"Download and extract MovieLens dataset\"\"\"\n",
        "\n",
        "        # Check if already downloaded\n",
        "        if os.path.exists(self.data_dir):\n",
        "            print(f\"✓ Dataset already exists at {self.data_dir}\")\n",
        "            return True\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"DOWNLOADING MOVIELENS DATASET\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Source: {self.url}\")\n",
        "        print(f\"Size: ~1 MB (100K ratings)\")\n",
        "\n",
        "        try:\n",
        "            # Download with progress\n",
        "            print(\"Downloading...\", end=\"\")\n",
        "            urllib.request.urlretrieve(self.url, self.zip_path)\n",
        "            print(\" ✓ Complete\")\n",
        "\n",
        "            # Extract\n",
        "            print(\"Extracting...\", end=\"\")\n",
        "            with zipfile.ZipFile(self.zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(\".\")\n",
        "            print(\" ✓ Complete\")\n",
        "\n",
        "            # Cleanup\n",
        "            os.remove(self.zip_path)\n",
        "            print(f\"✓ Dataset ready at {self.data_dir}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ Error: {e}\")\n",
        "            print(\"\\nManual download instructions:\")\n",
        "            print(f\"1. Visit: {self.url}\")\n",
        "            print(f\"2. Extract to: {self.data_dir}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PART 2: DATA LOADER WITH PERFECT JOINS\n",
        "# ============================================================================\n",
        "\n",
        "class MovieLensLoader:\n",
        "    \"\"\"\n",
        "    Loads MovieLens data with guaranteed 100% successful joins\n",
        "    No ID mismatch issues!\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_path: str):\n",
        "        self.data_path = data_path\n",
        "\n",
        "    def load_and_enrich(self, min_user_ratings: int = 5,\n",
        "                       min_movie_ratings: int = 5) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load all data files and create enriched dataset\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with columns:\n",
        "            - userId, movieId, rating, timestamp\n",
        "            - title, genres, primary_genre\n",
        "            - user_idx, item_idx (for matrix factorization)\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"LOADING MOVIELENS DATA\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Load ratings\n",
        "        print(\"\\n[1/3] Loading ratings.csv...\")\n",
        "        ratings_df = pd.read_csv(f\"{self.data_path}/ratings.csv\")\n",
        "        print(f\"✓ Loaded {len(ratings_df):,} ratings\")\n",
        "        print(f\"  → Users: {ratings_df['userId'].nunique():,}\")\n",
        "        print(f\"  → Movies: {ratings_df['movieId'].nunique():,}\")\n",
        "        print(f\"  → Rating range: {ratings_df['rating'].min():.1f} - {ratings_df['rating'].max():.1f}\")\n",
        "\n",
        "        # Load movies\n",
        "        print(\"\\n[2/3] Loading movies.csv...\")\n",
        "        movies_df = pd.read_csv(f\"{self.data_path}/movies.csv\")\n",
        "        print(f\"✓ Loaded {len(movies_df):,} movies\")\n",
        "\n",
        "        # Load tags (optional)\n",
        "        print(\"\\n[3/3] Loading tags.csv...\")\n",
        "        try:\n",
        "            tags_df = pd.read_csv(f\"{self.data_path}/tags.csv\")\n",
        "            # Aggregate tags per movie\n",
        "            movie_tags = tags_df.groupby('movieId')['tag'].apply(\n",
        "                lambda x: ' '.join(x.astype(str))\n",
        "            ).reset_index()\n",
        "            movie_tags.columns = ['movieId', 'tags']\n",
        "            print(f\"✓ Loaded {len(tags_df):,} tags\")\n",
        "            print(f\"  → Movies with tags: {len(movie_tags):,}\")\n",
        "        except:\n",
        "            movie_tags = pd.DataFrame(columns=['movieId', 'tags'])\n",
        "            print(\"⚠️  No tags file (optional)\")\n",
        "\n",
        "        # CRITICAL: Merge with 100% success\n",
        "        print(\"\\n🔗 Merging datasets...\")\n",
        "\n",
        "        # Merge ratings + movies\n",
        "        df = ratings_df.merge(movies_df, on='movieId', how='left')\n",
        "        merge_success = (df['title'].notna().sum() / len(df)) * 100\n",
        "        print(f\"  ✓ Ratings + Movies: {merge_success:.2f}% success (Expected: 100%)\")\n",
        "\n",
        "        if merge_success < 100:\n",
        "            missing = df['title'].isna().sum()\n",
        "            print(f\"  ⚠️  Warning: {missing:,} ratings without movie data\")\n",
        "            df = df.dropna(subset=['title'])\n",
        "\n",
        "        # Merge tags\n",
        "        if len(movie_tags) > 0:\n",
        "            df = df.merge(movie_tags, on='movieId', how='left')\n",
        "            df['tags'] = df['tags'].fillna('')\n",
        "        else:\n",
        "            df['tags'] = ''\n",
        "\n",
        "        # Process genres\n",
        "        df['genres_list'] = df['genres'].str.split('|')\n",
        "        df['primary_genre'] = df['genres_list'].str[0]\n",
        "        df['num_genres'] = df['genres_list'].str.len()\n",
        "\n",
        "        print(f\"\\n✓ Merged dataset: {len(df):,} ratings\")\n",
        "\n",
        "        # Filter sparse users/movies\n",
        "        print(f\"\\nFiltering (min {min_user_ratings} ratings/user, {min_movie_ratings} ratings/movie)...\")\n",
        "\n",
        "        # Filter users\n",
        "        user_counts = df.groupby('userId').size()\n",
        "        valid_users = user_counts[user_counts >= min_user_ratings].index\n",
        "        df = df[df['userId'].isin(valid_users)]\n",
        "\n",
        "        # Filter movies\n",
        "        movie_counts = df.groupby('movieId').size()\n",
        "        valid_movies = movie_counts[movie_counts >= min_movie_ratings].index\n",
        "        df = df[df['movieId'].isin(valid_movies)]\n",
        "\n",
        "        print(f\"  → Removed sparse users/movies\")\n",
        "        print(f\"  → Final dataset: {len(df):,} ratings\")\n",
        "\n",
        "        # Create indices for matrix factorization\n",
        "        df['user_idx'] = pd.Categorical(df['userId']).codes.astype(np.int32)\n",
        "        df['item_idx'] = pd.Categorical(df['movieId']).codes.astype(np.int32)\n",
        "\n",
        "        # Calculate statistics\n",
        "        n_users = df['user_idx'].nunique()\n",
        "        n_items = df['item_idx'].nunique()\n",
        "        sparsity = 1 - len(df) / (n_users * n_items)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"DATASET SUMMARY\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Total ratings:      {len(df):,}\")\n",
        "        print(f\"Unique users:       {n_users:,}\")\n",
        "        print(f\"Unique movies:      {n_items:,}\")\n",
        "        print(f\"Unique genres:      {df['primary_genre'].nunique()}\")\n",
        "        print(f\"Sparsity:           {sparsity:.6f} ({(1-sparsity)*100:.3f}% filled)\")\n",
        "        print(f\"Avg ratings/user:   {len(df)/n_users:.1f}\")\n",
        "        print(f\"Avg ratings/movie:  {len(df)/n_items:.1f}\")\n",
        "        print(f\"Rating distribution:\")\n",
        "        for rating, count in df['rating'].value_counts().sort_index().items():\n",
        "            pct = count / len(df) * 100\n",
        "            print(f\"  {rating:.1f}: {count:>6,} ({pct:>5.1f}%)\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PART 3: HYBRID MATRIX FACTORIZATION MODEL\n",
        "# ============================================================================\n",
        "\n",
        "class HybridMatrixFactorization:\n",
        "    \"\"\"\n",
        "    Hybrid Recommender: Collaborative Filtering + Content-Based\n",
        "\n",
        "    Features:\n",
        "    - Matrix factorization for collaborative signals\n",
        "    - Genre similarity for content-based signals\n",
        "    - Weighted hybrid predictions\n",
        "    - Early stopping to prevent overfitting\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_factors: int = 20, n_iterations: int = 50,\n",
        "                 reg_lambda: float = 0.1, learning_rate: float = 0.01,\n",
        "                 content_weight: float = 0.15, early_stopping: bool = True,\n",
        "                 patience: int = 10):\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.n_factors = n_factors\n",
        "        self.n_iterations = n_iterations\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.learning_rate = learning_rate\n",
        "        self.content_weight = content_weight\n",
        "        self.cf_weight = 1 - content_weight\n",
        "        self.early_stopping = early_stopping\n",
        "        self.patience = patience\n",
        "\n",
        "        # Model parameters\n",
        "        self.P = None  # User latent factors\n",
        "        self.Q = None  # Item latent factors\n",
        "        self.user_bias = None\n",
        "        self.item_bias = None\n",
        "        self.global_mean = 0\n",
        "\n",
        "        # Content features\n",
        "        self.item_to_genre = {}\n",
        "        self.item_to_genres = {}\n",
        "\n",
        "        # Training history\n",
        "        self.train_errors = []\n",
        "        self.val_errors = []\n",
        "\n",
        "        # Best model (for early stopping)\n",
        "        self.best_state = None\n",
        "        self.best_val_rmse = float('inf')\n",
        "        self.best_epoch = 0\n",
        "\n",
        "    def build_content_features(self, data: pd.DataFrame):\n",
        "        \"\"\"Build genre-based content similarity\"\"\"\n",
        "        print(\"\\n--- Building Content Features ---\")\n",
        "\n",
        "        # Map item to primary genre\n",
        "        self.item_to_genre = data.groupby('item_idx')['primary_genre'].first().to_dict()\n",
        "\n",
        "        # Map item to all genres (for richer similarity)\n",
        "        self.item_to_genres = data.groupby('item_idx')['genres_list'].first().to_dict()\n",
        "\n",
        "        n_genres = data['primary_genre'].nunique()\n",
        "        n_items_with_genre = len(self.item_to_genre)\n",
        "\n",
        "        print(f\"✓ Unique genres: {n_genres}\")\n",
        "        print(f\"✓ Items with genre info: {n_items_with_genre:,}\")\n",
        "\n",
        "    def calculate_genre_similarity(self, item_idx1: int, item_idx2: int) -> float:\n",
        "        \"\"\"\n",
        "        Calculate genre similarity between two items\n",
        "\n",
        "        Uses Jaccard similarity on genre sets:\n",
        "        similarity = |intersection| / |union|\n",
        "        \"\"\"\n",
        "        genres1 = set(self.item_to_genres.get(item_idx1, []))\n",
        "        genres2 = set(self.item_to_genres.get(item_idx2, []))\n",
        "\n",
        "        if not genres1 or not genres2:\n",
        "            return 0.0\n",
        "\n",
        "        intersection = len(genres1 & genres2)\n",
        "        union = len(genres1 | genres2)\n",
        "\n",
        "        return intersection / union if union > 0 else 0.0\n",
        "\n",
        "    def initialize_parameters(self, n_users: int, n_items: int):\n",
        "        \"\"\"Initialize model parameters with small random values\"\"\"\n",
        "        print(\"\\n--- Initializing Model Parameters ---\")\n",
        "\n",
        "        self.P = np.random.normal(0, 0.1, (n_users, self.n_factors)).astype(np.float32)\n",
        "        self.Q = np.random.normal(0, 0.1, (n_items, self.n_factors)).astype(np.float32)\n",
        "        self.user_bias = np.zeros(n_users, dtype=np.float32)\n",
        "        self.item_bias = np.zeros(n_items, dtype=np.float32)\n",
        "\n",
        "        print(f\"✓ User factors (P): {self.P.shape}\")\n",
        "        print(f\"✓ Item factors (Q): {self.Q.shape}\")\n",
        "        total_params = int(self.P.size) + int(self.Q.size) + int(n_users) + int(n_items)\n",
        "        print(f\"✓ Parameters: {total_params:,}\")\n",
        "\n",
        "    def predict_cf(self, user_idx: int, item_idx: int) -> float:\n",
        "        \"\"\"Collaborative filtering prediction only\"\"\"\n",
        "        if user_idx >= len(self.user_bias) or item_idx >= len(self.item_bias):\n",
        "            return self.global_mean\n",
        "\n",
        "        prediction = (\n",
        "            self.global_mean +\n",
        "            self.user_bias[user_idx] +\n",
        "            self.item_bias[item_idx] +\n",
        "            np.dot(self.P[user_idx], self.Q[item_idx])\n",
        "        )\n",
        "\n",
        "        return prediction\n",
        "\n",
        "    def predict_content(self, user_idx: int, item_idx: int,\n",
        "                       user_items: np.ndarray, user_ratings: np.ndarray) -> float:\n",
        "        \"\"\"Content-based prediction using genre similarity\"\"\"\n",
        "        if len(user_items) == 0:\n",
        "            return self.global_mean\n",
        "\n",
        "        # Calculate similarity with all items user has rated\n",
        "        similarities = np.array([\n",
        "            self.calculate_genre_similarity(item_idx, int(rated_item))\n",
        "            for rated_item in user_items\n",
        "        ])\n",
        "\n",
        "        # Weighted average of ratings by similarity\n",
        "        if similarities.sum() > 0:\n",
        "            weights = similarities / similarities.sum()\n",
        "            prediction = np.dot(weights, user_ratings)\n",
        "            return prediction\n",
        "\n",
        "        return self.global_mean\n",
        "\n",
        "    def predict_hybrid(self, user_idx: int, item_idx: int,\n",
        "                      user_items: np.ndarray = None,\n",
        "                      user_ratings: np.ndarray = None) -> float:\n",
        "        \"\"\"\n",
        "        Hybrid prediction combining CF and content-based\n",
        "\n",
        "        Final prediction = cf_weight * CF + content_weight * CB\n",
        "        \"\"\"\n",
        "        # Collaborative filtering component\n",
        "        cf_prediction = self.predict_cf(user_idx, item_idx)\n",
        "\n",
        "        # Content-based component (if user history available)\n",
        "        if user_items is not None and len(user_items) > 0:\n",
        "            cb_prediction = self.predict_content(\n",
        "                user_idx, item_idx, user_items, user_ratings\n",
        "            )\n",
        "            # Weighted combination\n",
        "            prediction = (\n",
        "                self.cf_weight * cf_prediction +\n",
        "                self.content_weight * cb_prediction\n",
        "            )\n",
        "        else:\n",
        "            prediction = cf_prediction\n",
        "\n",
        "        # Clip to valid rating range\n",
        "        return np.clip(prediction, 0.5, 5.0)\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        \"\"\"Save current model state\"\"\"\n",
        "        self.best_state = {\n",
        "            'P': self.P.copy(),\n",
        "            'Q': self.Q.copy(),\n",
        "            'user_bias': self.user_bias.copy(),\n",
        "            'item_bias': self.item_bias.copy()\n",
        "        }\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        \"\"\"Restore best model state\"\"\"\n",
        "        if self.best_state is not None:\n",
        "            self.P = self.best_state['P']\n",
        "            self.Q = self.best_state['Q']\n",
        "            self.user_bias = self.best_state['user_bias']\n",
        "            self.item_bias = self.best_state['item_bias']\n",
        "\n",
        "    def fit(self, train_data: pd.DataFrame, val_data: pd.DataFrame = None):\n",
        "        \"\"\"\n",
        "        Train the hybrid model using SGD\n",
        "\n",
        "        Args:\n",
        "            train_data: Training ratings\n",
        "            val_data: Validation ratings (for early stopping)\n",
        "        \"\"\"\n",
        "\n",
        "        # Build content features\n",
        "        self.build_content_features(train_data)\n",
        "\n",
        "        # Extract training data\n",
        "        users = train_data['user_idx'].values.astype(np.int32)\n",
        "        items = train_data['item_idx'].values.astype(np.int32)\n",
        "        ratings = train_data['rating'].values.astype(np.float32)\n",
        "\n",
        "        # Determine matrix dimensions\n",
        "        n_users = int(max(\n",
        "            train_data['user_idx'].max() + 1,\n",
        "            val_data['user_idx'].max() + 1 if val_data is not None else 0\n",
        "        ))\n",
        "        n_items = int(max(\n",
        "            train_data['item_idx'].max() + 1,\n",
        "            val_data['item_idx'].max() + 1 if val_data is not None else 0\n",
        "        ))\n",
        "\n",
        "        # Initialize\n",
        "        self.global_mean = ratings.mean()\n",
        "        self.initialize_parameters(n_users, n_items)\n",
        "\n",
        "        # Build user profiles for predictions\n",
        "        print(\"Building user profiles...\")\n",
        "        user_profiles = {}\n",
        "        for user_idx in np.unique(users):\n",
        "            mask = users == user_idx\n",
        "            user_profiles[int(user_idx)] = (\n",
        "                items[mask].astype(np.int32),\n",
        "                ratings[mask].astype(np.float32)\n",
        "            )\n",
        "        print(f\"✓ Built profiles for {len(user_profiles):,} users\")\n",
        "\n",
        "        # Training loop\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"TRAINING HYBRID MODEL\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Architecture: {n_users:,} users × {n_items:,} items × {self.n_factors} factors\")\n",
        "        print(f\"Training samples: {len(train_data):,}\")\n",
        "        if val_data is not None:\n",
        "            print(f\"Validation samples: {len(val_data):,}\")\n",
        "        print(f\"Hybrid weights: CF={self.cf_weight:.0%}, Content={self.content_weight:.0%}\")\n",
        "        print(f\"Regularization: λ={self.reg_lambda}\")\n",
        "        print(f\"Learning rate: α={self.learning_rate}\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(self.n_iterations):\n",
        "            # Shuffle training data\n",
        "            indices = np.random.permutation(len(users))\n",
        "\n",
        "            # SGD updates\n",
        "            for idx in indices:\n",
        "                u, i, r = users[idx], items[idx], ratings[idx]\n",
        "\n",
        "                # Predict and calculate error\n",
        "                user_items, user_ratings_hist = user_profiles.get(u, (np.array([]), np.array([])))\n",
        "                prediction = self.predict_hybrid(u, i, user_items, user_ratings_hist)\n",
        "                error = r - prediction\n",
        "\n",
        "                # Update biases\n",
        "                self.user_bias[u] += self.learning_rate * (error - self.reg_lambda * self.user_bias[u])\n",
        "                self.item_bias[i] += self.learning_rate * (error - self.reg_lambda * self.item_bias[i])\n",
        "\n",
        "                # Update latent factors\n",
        "                self.P[u, :] += self.learning_rate * (error * self.Q[i, :] - self.reg_lambda * self.P[u, :])\n",
        "                self.Q[i, :] += self.learning_rate * (error * self.P[u, :] - self.reg_lambda * self.Q[i, :])\n",
        "\n",
        "            # Evaluate training performance\n",
        "            train_predictions = []\n",
        "            for u, i in zip(users, items):\n",
        "                user_items, user_ratings = user_profiles.get(u, (np.array([]), np.array([])))\n",
        "                pred = self.predict_hybrid(u, i, user_items, user_ratings)\n",
        "                train_predictions.append(pred)\n",
        "\n",
        "            train_rmse = np.sqrt(mean_squared_error(ratings, train_predictions))\n",
        "            self.train_errors.append(train_rmse)\n",
        "\n",
        "            # Evaluate validation performance\n",
        "            if val_data is not None:\n",
        "                val_users = val_data['user_idx'].values.astype(np.int32)\n",
        "                val_items = val_data['item_idx'].values.astype(np.int32)\n",
        "                val_ratings = val_data['rating'].values.astype(np.float32)\n",
        "\n",
        "                val_predictions = []\n",
        "                for u, i in zip(val_users, val_items):\n",
        "                    user_items, user_ratings = user_profiles.get(u, (np.array([]), np.array([])))\n",
        "                    pred = self.predict_hybrid(u, i, user_items, user_ratings)\n",
        "                    val_predictions.append(pred)\n",
        "\n",
        "                val_rmse = np.sqrt(mean_squared_error(val_ratings, val_predictions))\n",
        "                self.val_errors.append(val_rmse)\n",
        "\n",
        "                # Print progress\n",
        "                if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "                    gap = val_rmse - train_rmse\n",
        "                    status = \"✓\" if val_rmse < self.best_val_rmse else \"✗\"\n",
        "                    print(f\"Epoch {epoch+1:3d}/{self.n_iterations} | \"\n",
        "                          f\"Train: {train_rmse:.4f} | Val: {val_rmse:.4f} | \"\n",
        "                          f\"Gap: {gap:+.4f} {status}\")\n",
        "\n",
        "                # Early stopping\n",
        "                if self.early_stopping:\n",
        "                    if val_rmse < self.best_val_rmse:\n",
        "                        self.best_val_rmse = val_rmse\n",
        "                        self.best_epoch = epoch\n",
        "                        patience_counter = 0\n",
        "                        self.save_checkpoint()\n",
        "                    else:\n",
        "                        patience_counter += 1\n",
        "                        if patience_counter >= self.patience:\n",
        "                            print(\"\\n\" + \"─\"*70)\n",
        "                            print(f\"⏹  Early stopping at epoch {epoch+1}\")\n",
        "                            print(f\"   Best validation RMSE: {self.best_val_rmse:.4f} (epoch {self.best_epoch+1})\")\n",
        "                            print(\"─\"*70)\n",
        "                            self.load_checkpoint()\n",
        "                            break\n",
        "            else:\n",
        "                # No validation set - just print training progress\n",
        "                if (epoch + 1) % 10 == 0:\n",
        "                    print(f\"Epoch {epoch+1}/{self.n_iterations} | Train RMSE: {train_rmse:.4f}\")\n",
        "\n",
        "\n",
        "        print(\"\\n✓ Training completed!\")\n",
        "        if val_data is not None and self.early_stopping:\n",
        "            print(f\"✓ Best model from epoch {self.best_epoch+1} (Val RMSE: {self.best_val_rmse:.4f})\")\n",
        "\n",
        "\n",
        "    def get_top_recommendations(self, user_idx: int, n: int,\n",
        "                               user_items: np.ndarray, user_ratings: np.ndarray,\n",
        "                               rated_items: Set[int]) -> List[Tuple[int, float]]:\n",
        "        \"\"\"\n",
        "        Get top N movie recommendations for a user\n",
        "\n",
        "        Args:\n",
        "            user_idx: User index\n",
        "            n: Number of recommendations\n",
        "            user_items: Items user has rated\n",
        "            user_ratings: Ratings user has given\n",
        "            rated_items: Set of already rated items (to exclude)\n",
        "\n",
        "        Returns:\n",
        "            List of (item_idx, predicted_rating) tuples\n",
        "        \"\"\"\n",
        "        n_items = self.Q.shape[0]\n",
        "        predictions = []\n",
        "\n",
        "        for item_idx in range(n_items):\n",
        "            if item_idx in rated_items:\n",
        "                continue\n",
        "\n",
        "            pred = self.predict_hybrid(user_idx, item_idx, user_items, user_ratings)\n",
        "            predictions.append((item_idx, pred))\n",
        "\n",
        "        # Sort by predicted rating (descending)\n",
        "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return predictions[:n]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PART 4: RECOMMENDER INTERFACE\n",
        "# ============================================================================\n",
        "\n",
        "class MovieRecommender:\n",
        "    \"\"\"User-friendly interface for getting recommendations\"\"\"\n",
        "\n",
        "    def __init__(self, model: HybridMatrixFactorization, data: pd.DataFrame):\n",
        "        self.model = model\n",
        "        self.data = data\n",
        "\n",
        "        # Create mappings\n",
        "        self.user_id_to_idx = dict(zip(data['userId'], data['user_idx']))\n",
        "        self.idx_to_movie_id = dict(zip(data['item_idx'], data['movieId']))\n",
        "\n",
        "        # Movie information\n",
        "        self.movie_info = data.groupby('item_idx').agg({\n",
        "            'movieId': 'first',\n",
        "            'title': 'first',\n",
        "            'genres': 'first',\n",
        "            'primary_genre': 'first',\n",
        "            'rating': 'mean'\n",
        "        }).to_dict('index')\n",
        "\n",
        "        # Build user profiles\n",
        "        self.user_profiles = {}\n",
        "        for user_idx in data['user_idx'].unique():\n",
        "            user_data = data[data['user_idx'] == user_idx]\n",
        "            self.user_profiles[user_idx] = (\n",
        "                user_data['item_idx'].values,\n",
        "                user_data['rating'].values\n",
        "            )\n",
        "\n",
        "    def recommend(self, user_id: int, n: int = 10) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Get top N recommendations for a user\n",
        "\n",
        "        Args:\n",
        "            user_id: User ID (from original dataset)\n",
        "            n: Number of recommendations\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with recommendations\n",
        "        \"\"\"\n",
        "        if user_id not in self.user_id_to_idx:\n",
        "            return self._popular_recommendations(n)\n",
        "\n",
        "        user_idx = self.user_id_to_idx[user_id]\n",
        "        rated_items = set(self.data[self.data['userId'] == user_id]['item_idx'])\n",
        "        user_items, user_ratings = self.user_profiles.get(\n",
        "            user_idx,\n",
        "            (np.array([]), np.array([]))\n",
        "        )\n",
        "\n",
        "        recommendations = self.model.get_top_recommendations(\n",
        "            user_idx, n, user_items, user_ratings, rated_items\n",
        "        )\n",
        "\n",
        "        results = []\n",
        "        for item_idx, predicted_rating in recommendations:\n",
        "            info = self.movie_info[item_idx]\n",
        "            results.append({\n",
        "                'movieId': info['movieId'],\n",
        "                'title': info['title'],\n",
        "                'genres': info['genres'],\n",
        "                'predicted_rating': round(predicted_rating, 2),\n",
        "                'avg_rating': round(info['rating'], 2)\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    def _popular_recommendations(self, n: int) -> pd.DataFrame:\n",
        "        \"\"\"Fallback: return most popular movies (cold start)\"\"\"\n",
        "        popular = self.data.groupby('item_idx').agg({\n",
        "            'rating': ['mean', 'count'],\n",
        "            'movieId': 'first',\n",
        "            'title': 'first',\n",
        "            'genres': 'first'\n",
        "        })\n",
        "\n",
        "        # Weight by rating and popularity\n",
        "        popular['score'] = popular[('rating', 'mean')] * np.log1p(popular[('rating', 'count')])\n",
        "        popular = popular.sort_values('score', ascending=False).head(n)\n",
        "\n",
        "        results = []\n",
        "        for _, row in popular.iterrows():\n",
        "            results.append({\n",
        "                'movieId': row[('movieId', 'first')],\n",
        "                'title': row[('title', 'first')],\n",
        "                'genres': row[('genres', 'first')],\n",
        "                'predicted_rating': round(row[('rating', 'mean')], 2),\n",
        "                'avg_rating': round(row[('rating', 'mean')], 2)\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    def get_user_history(self, user_id: int) -> pd.DataFrame:\n",
        "        \"\"\"Get user's rating history\"\"\"\n",
        "        return self.data[self.data['userId'] == user_id][\n",
        "            ['title', 'genres', 'rating', 'primary_genre']\n",
        "        ].sort_values('rating', ascending=False)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PART 5: VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def plot_learning_curves(train_errors: List[float], val_errors: List[float],\n",
        "                        save_path: str = 'learning_curves.png'):\n",
        "    \"\"\"Plot training and validation learning curves\"\"\"\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    epochs = range(1, len(train_errors) + 1)\n",
        "\n",
        "    plt.plot(epochs, train_errors, 'o-', label='Train RMSE',\n",
        "             linewidth=2, markersize=4, color='#2E86AB', alpha=0.8)\n",
        "\n",
        "    if val_errors:\n",
        "        plt.plot(epochs, val_errors, 's-', label='Validation RMSE',\n",
        "                linewidth=2, markersize=4, color='#A23B72', alpha=0.8)\n",
        "\n",
        "        # Highlight best epoch\n",
        "        best_epoch = np.argmin(val_errors) + 1\n",
        "        best_val = np.min(val_errors)\n",
        "        plt.axvline(x=best_epoch, color='red', linestyle='--', alpha=0.4,\n",
        "                   label=f'Best Epoch ({best_epoch})')\n",
        "        plt.scatter(best_epoch, best_val, color='red', s=150, zorder=5,\n",
        "                   marker='*', edgecolors='darkred', linewidths=2)\n",
        "\n",
        "        # Add text annotation\n",
        "        plt.text(best_epoch, best_val - 0.02, f'RMSE: {best_val:.4f}',\n",
        "                ha='center', va='top', fontsize=10, fontweight='bold',\n",
        "                bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))\n",
        "\n",
        "    plt.xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('RMSE', fontsize=12, fontweight='bold')\n",
        "    plt.title('Hybrid Model Learning Curves', fontsize=14, fontweight='bold')\n",
        "    plt.legend(fontsize=10, loc='upper right')\n",
        "    plt.grid(True, alpha=0.3, linestyle='--')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"\\n✓ Saved learning curves: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_data_statistics(data: pd.DataFrame, save_path: str = 'data_statistics.png'):\n",
        "    \"\"\"Plot dataset statistics\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    # 1. Rating distribution\n",
        "    rating_counts = data['rating'].value_counts().sort_index()\n",
        "    axes[0, 0].bar(rating_counts.index, rating_counts.values,\n",
        "                   color='steelblue', edgecolor='black', alpha=0.8)\n",
        "    axes[0, 0].set_title('Rating Distribution', fontsize=12, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Rating')\n",
        "    axes[0, 0].set_ylabel('Count')\n",
        "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # 2. Top genres\n",
        "    top_genres = data['primary_genre'].value_counts().head(15)\n",
        "    axes[0, 1].barh(range(len(top_genres)), top_genres.values, color='coral', edgecolor='black')\n",
        "    axes[0, 1].set_yticks(range(len(top_genres)))\n",
        "    axes[0, 1].set_yticklabels(top_genres.index)\n",
        "    axes[0, 1].set_title('Top 15 Genres', fontsize=12, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Number of Ratings')\n",
        "    axes[0, 1].invert_yaxis()\n",
        "    axes[0, 1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "    # 3. Ratings per user\n",
        "    user_rating_counts = data.groupby('user_idx').size()\n",
        "    axes[1, 0].hist(user_rating_counts, bins=50, edgecolor='black',\n",
        "                    color='lightgreen', alpha=0.8)\n",
        "    axes[1, 0].set_title('Ratings per User', fontsize=12, fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Number of Ratings')\n",
        "    axes[1, 0].set_ylabel('Number of Users (log scale)')\n",
        "    axes[1, 0].set_yscale('log')\n",
        "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # 4. Ratings per movie\n",
        "    movie_rating_counts = data.groupby('item_idx').size()\n",
        "    axes[1, 1].hist(movie_rating_counts, bins=50, edgecolor='black',\n",
        "                    color='plum', alpha=0.8)\n",
        "    axes[1, 1].set_title('Ratings per Movie', fontsize=12, fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Number of Ratings')\n",
        "    axes[1, 1].set_ylabel('Number of Movies (log scale)')\n",
        "    axes[1, 1].set_yscale('log')\n",
        "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"✓ Saved data statistics: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_predictions_vs_actual(train_data: pd.DataFrame, test_data: pd.DataFrame,\n",
        "                                model: HybridMatrixFactorization,\n",
        "                                user_profiles: Dict,\n",
        "                                save_path: str = 'predictions_analysis.png'):\n",
        "    \"\"\"Plot prediction quality analysis\"\"\"\n",
        "\n",
        "    # Get predictions for test set\n",
        "    test_users = test_data['user_idx'].values\n",
        "    test_items = test_data['item_idx'].values\n",
        "    test_ratings = test_data['rating'].values\n",
        "\n",
        "    test_predictions = []\n",
        "    for u, i in zip(test_users, test_items):\n",
        "        user_items, user_ratings_hist = user_profiles.get(u, (np.array([]), np.array([])))\n",
        "        pred = model.predict_hybrid(u, i, user_items, user_ratings_hist)\n",
        "        test_predictions.append(pred)\n",
        "\n",
        "    test_predictions = np.array(test_predictions)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # 1. Scatter plot: Predicted vs Actual\n",
        "    axes[0].scatter(test_ratings, test_predictions, alpha=0.3, s=10, color='steelblue')\n",
        "    axes[0].plot([0.5, 5], [0.5, 5], 'r--', linewidth=2, label='Perfect predictions')\n",
        "    axes[0].set_xlabel('Actual Rating', fontsize=12, fontweight='bold')\n",
        "    axes[0].set_ylabel('Predicted Rating', fontsize=12, fontweight='bold')\n",
        "    axes[0].set_title('Predicted vs Actual Ratings', fontsize=12, fontweight='bold')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].set_xlim(0.5, 5.5)\n",
        "    axes[0].set_ylim(0.5, 5.5)\n",
        "\n",
        "    # 2. Error distribution\n",
        "    errors = test_ratings - test_predictions\n",
        "    axes[1].hist(errors, bins=50, edgecolor='black', color='coral', alpha=0.8)\n",
        "    axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero error')\n",
        "    axes[1].set_xlabel('Prediction Error', fontsize=12, fontweight='bold')\n",
        "    axes[1].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
        "    axes[1].set_title('Prediction Error Distribution', fontsize=12, fontweight='bold')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add statistics text\n",
        "    mae = np.mean(np.abs(errors))\n",
        "    rmse = np.sqrt(np.mean(errors**2))\n",
        "    stats_text = f'MAE: {mae:.4f}\\nRMSE: {rmse:.4f}\\nMean Error: {np.mean(errors):.4f}'\n",
        "    axes[1].text(0.02, 0.98, stats_text, transform=axes[1].transAxes,\n",
        "                fontsize=10, verticalalignment='top',\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"✓ Saved prediction analysis: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PART 6: MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Complete pipeline: Download → Load → Train → Evaluate → Recommend\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"█\"*70)\n",
        "    print(\"   COMPLETE MOVIELENS HYBRID RECOMMENDER SYSTEM\")\n",
        "    print(\"   GitHub Codespaces Ready - Auto Setup\")\n",
        "    print(\"█\"*70)\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 1: AUTO-DOWNLOAD DATASET\n",
        "    # ========================================================================\n",
        "\n",
        "    setup = DatasetSetup()\n",
        "    if not setup.download_and_extract():\n",
        "        print(\"\\n❌ Setup failed. Please download manually.\")\n",
        "        return None\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 2: LOAD AND PREPARE DATA\n",
        "    # ========================================================================\n",
        "\n",
        "    loader = MovieLensLoader(setup.data_dir)\n",
        "    data = loader.load_and_enrich(min_user_ratings=5, min_movie_ratings=5)\n",
        "\n",
        "    # Save dataset statistics\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"GENERATING VISUALIZATIONS\")\n",
        "    print(\"=\"*70)\n",
        "    plot_data_statistics(data)\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 3: TRAIN/VALIDATION/TEST SPLIT\n",
        "    # ========================================================================\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"TRAIN/VALIDATION/TEST SPLIT (60/20/20)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Split: 60% train, 20% validation, 20% test\n",
        "    train_df, temp_df = train_test_split(data, test_size=0.4, random_state=42)\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "    print(f\"Train:      {len(train_df):>7,} ratings ({len(train_df)/len(data)*100:>5.1f}%)\")\n",
        "    print(f\"Validation: {len(val_df):>7,} ratings ({len(val_df)/len(data)*100:>5.1f}%)\")\n",
        "    print(f\"Test:       {len(test_df):>7,} ratings ({len(test_df)/len(data)*100:>5.1f}%)\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 4: TRAIN HYBRID MODEL\n",
        "    # ========================================================================\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"INITIALIZING HYBRID MODEL\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    model = HybridMatrixFactorization(\n",
        "        n_factors=20,           # Number of latent factors\n",
        "        n_iterations=60,        # Max training epochs\n",
        "        reg_lambda=0.25,         # Regularization strength\n",
        "        learning_rate=0.005,     # Learning rate\n",
        "        content_weight=0.15,    # 15% content, 85% collaborative\n",
        "        early_stopping=True,    # Enable early stopping\n",
        "        patience=10             # Stop after 10 epochs without improvement\n",
        "    )\n",
        "\n",
        "    print(\"Hyperparameters:\")\n",
        "    print(f\"  • Latent factors: {model.n_factors}\")\n",
        "    print(f\"  • Max iterations: {model.n_iterations}\")\n",
        "    print(f\"  • Regularization: {model.reg_lambda}\")\n",
        "    print(f\"  • Learning rate: {model.learning_rate}\")\n",
        "    print(f\"  • Content weight: {model.content_weight:.0%}\")\n",
        "    print(f\"  • CF weight: {model.cf_weight:.0%}\")\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(train_df, val_df)\n",
        "\n",
        "    # Plot learning curves\n",
        "    plot_learning_curves(model.train_errors, model.val_errors)\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 5: FINAL EVALUATION ON TEST SET\n",
        "    # ========================================================================\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"FINAL EVALUATION ON TEST SET\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Build user profiles for predictions\n",
        "    user_profiles = {}\n",
        "    for user_idx in train_df['user_idx'].unique():\n",
        "        user_data = train_df[train_df['user_idx'] == user_idx]\n",
        "        user_profiles[user_idx] = (\n",
        "            user_data['item_idx'].values,\n",
        "            user_data['rating'].values\n",
        "        )\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"Generating test predictions...\")\n",
        "    test_users = test_df['user_idx'].values\n",
        "    test_items = test_df['item_idx'].values\n",
        "    test_ratings = test_df['rating'].values\n",
        "\n",
        "    test_predictions = []\n",
        "    for u, i in zip(test_users, test_items):\n",
        "        user_items, user_ratings_hist = user_profiles.get(u, (np.array([]), np.array([])))\n",
        "        pred = model.predict_hybrid(u, i, user_items, user_ratings_hist)\n",
        "        test_predictions.append(pred)\n",
        "\n",
        "    test_predictions = np.array(test_predictions)\n",
        "\n",
        "    # Calculate metrics\n",
        "    test_rmse = np.sqrt(mean_squared_error(test_ratings, test_predictions))\n",
        "    test_mae = mean_absolute_error(test_ratings, test_predictions)\n",
        "\n",
        "    # Also calculate for train and validation for comparison\n",
        "    print(\"Evaluating all sets...\")\n",
        "\n",
        "    train_preds = []\n",
        "    for u, i in zip(train_df['user_idx'].values, train_df['item_idx'].values):\n",
        "        user_items, user_ratings_hist = user_profiles.get(u, (np.array([]), np.array([])))\n",
        "        train_preds.append(model.predict_hybrid(u, i, user_items, user_ratings_hist))\n",
        "    train_rmse = np.sqrt(mean_squared_error(train_df['rating'].values, train_preds))\n",
        "    train_mae = mean_absolute_error(train_df['rating'].values, train_preds)\n",
        "\n",
        "    val_preds = []\n",
        "    for u, i in zip(val_df['user_idx'].values, val_df['item_idx'].values):\n",
        "        user_items, user_ratings_hist = user_profiles.get(u, (np.array([]), np.array([])))\n",
        "        val_preds.append(model.predict_hybrid(u, i, user_items, user_ratings_hist))\n",
        "    val_rmse = np.sqrt(mean_squared_error(val_df['rating'].values, val_preds))\n",
        "    val_mae = mean_absolute_error(val_df['rating'].values, val_preds)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"FINAL RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"{'Dataset':<12} {'RMSE':>10} {'MAE':>10} {'Samples':>12}\")\n",
        "    print(\"─\"*70)\n",
        "    print(f\"{'Train':<12} {train_rmse:>10.4f} {train_mae:>10.4f} {len(train_df):>12,}\")\n",
        "    print(f\"{'Validation':<12} {val_rmse:>10.4f} {val_mae:>10.4f} {len(val_df):>12,}\")\n",
        "    print(f\"{'Test':<12} {test_rmse:>10.4f} {test_mae:>10.4f} {len(test_df):>12,}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Performance analysis\n",
        "    print(\"\\n📊 PERFORMANCE ANALYSIS:\")\n",
        "    train_val_gap = val_rmse - train_rmse\n",
        "    val_test_diff = abs(test_rmse - val_rmse)\n",
        "\n",
        "    print(f\"\\n  Train-Val Gap: {train_val_gap:+.4f}\")\n",
        "    if train_val_gap < 0.05:\n",
        "        print(\"    ✅ Excellent! Minimal overfitting\")\n",
        "    elif train_val_gap < 0.10:\n",
        "        print(\"    ✅ Good generalization\")\n",
        "    elif train_val_gap < 0.15:\n",
        "        print(\"    ⚠️  Moderate overfitting\")\n",
        "    else:\n",
        "        print(\"    ❌ Significant overfitting - consider more regularization\")\n",
        "\n",
        "    print(f\"\\n  Val-Test Difference: {val_test_diff:.4f}\")\n",
        "    if val_test_diff < 0.02:\n",
        "        print(\"    ✅ Excellent! Validation accurately predicts test performance\")\n",
        "    elif val_test_diff < 0.04:\n",
        "        print(\"    ✅ Good validation reliability\")\n",
        "    else:\n",
        "        print(\"    ⚠️  Some variance between validation and test\")\n",
        "\n",
        "    # Plot prediction analysis\n",
        "    plot_predictions_vs_actual(train_df, test_df, model, user_profiles)\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 6: CREATE RECOMMENDER SYSTEM\n",
        "    # ========================================================================\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"CREATING RECOMMENDATION INTERFACE\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    recommender = MovieRecommender(model, data)\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 7: GENERATE SAMPLE RECOMMENDATIONS\n",
        "    # ========================================================================\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"SAMPLE RECOMMENDATIONS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Get diverse sample users (different rating patterns)\n",
        "    user_rating_counts = data.groupby('userId').size().sort_values(ascending=False)\n",
        "\n",
        "    # Heavy user, moderate user, light user\n",
        "    sample_users = [\n",
        "        user_rating_counts.index[10],    # Heavy user\n",
        "        user_rating_counts.index[100],   # Moderate user\n",
        "        user_rating_counts.index[500]    # Light user\n",
        "    ]\n",
        "\n",
        "    for i, user_id in enumerate(sample_users, 1):\n",
        "        user_count = user_rating_counts[user_id]\n",
        "\n",
        "        print(f\"\\n{'═'*70}\")\n",
        "        print(f\"SAMPLE USER #{i}: ID={user_id} ({user_count} ratings)\")\n",
        "        print(f\"{'═'*70}\")\n",
        "\n",
        "        # Get user history\n",
        "        history = recommender.get_user_history(user_id)\n",
        "\n",
        "        print(f\"\\n📚 Rating History (Top 5):\")\n",
        "        print(history.head(5).to_string(index=False))\n",
        "\n",
        "        # Get recommendations\n",
        "        recommendations = recommender.recommend(user_id, n=10)\n",
        "\n",
        "        print(f\"\\n🎯 Top 10 Recommendations:\")\n",
        "        print(recommendations.to_string(index=False))\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 8: SYSTEM SUMMARY\n",
        "    # ========================================================================\n",
        "\n",
        "    print(\"\\n\" + \"█\"*70)\n",
        "    print(\"✅ SYSTEM EXECUTION COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"█\"*70)\n",
        "\n",
        "    print(\"\\n📊 SYSTEM SUMMARY:\")\n",
        "    print(f\"  • Total ratings processed: {len(data):,}\")\n",
        "    print(f\"  • Unique users: {data['user_idx'].nunique():,}\")\n",
        "    print(f\"  • Unique movies: {data['item_idx'].nunique():,}\")\n",
        "    print(f\"  • Genres available: {data['primary_genre'].nunique()}\")\n",
        "    print(f\"  • Data sparsity: {1 - len(data)/(data['user_idx'].nunique() * data['item_idx'].nunique()):.6f}\")\n",
        "\n",
        "    print(\"\\n🎯 MODEL PERFORMANCE:\")\n",
        "    print(f\"  • Test RMSE: {test_rmse:.4f}\")\n",
        "    print(f\"  • Test MAE: {test_mae:.4f}\")\n",
        "    print(f\"  • Training epochs: {len(model.train_errors)}\")\n",
        "    if model.early_stopping and model.best_epoch > 0:\n",
        "        print(f\"  • Best epoch: {model.best_epoch + 1}\")\n",
        "\n",
        "    print(\"\\n🔧 HYBRID ARCHITECTURE:\")\n",
        "    print(f\"  • Collaborative filtering: {model.cf_weight:.0%}\")\n",
        "    print(f\"  • Content-based: {model.content_weight:.0%}\")\n",
        "    print(f\"  • Latent factors: {model.n_factors}\")\n",
        "    print(f\"  • Total parameters: {model.P.size + model.Q.size + len(model.user_bias) + len(model.item_bias):,}\")\n",
        "\n",
        "    print(\"\\n📁 OUTPUT FILES:\")\n",
        "    print(\"  ✓ data_statistics.png - Dataset overview\")\n",
        "    print(\"  ✓ learning_curves.png - Training progress\")\n",
        "    print(\"  ✓ predictions_analysis.png - Prediction quality\")\n",
        "\n",
        "    print(\"\\n💡 USAGE EXAMPLES:\")\n",
        "    print(\"  # Get recommendations for a user\")\n",
        "    print(\"  recommendations = recommender.recommend(user_id=42, n=10)\")\n",
        "    print(\"\")\n",
        "    print(\"  # View user's rating history\")\n",
        "    print(\"  history = recommender.get_user_history(user_id=42)\")\n",
        "    print(\"\")\n",
        "    print(\"  # Make a prediction\")\n",
        "    print(\"  prediction = model.predict_hybrid(user_idx=5, item_idx=100)\")\n",
        "\n",
        "    print(\"\\n🚀 NEXT STEPS:\")\n",
        "    print(\"  1. Experiment with hyperparameters (n_factors, reg_lambda)\")\n",
        "    print(\"  2. Try different content features (tags, timestamps)\")\n",
        "    print(\"  3. Implement user-user or item-item collaborative filtering\")\n",
        "    print(\"  4. Add confidence intervals for predictions\")\n",
        "    print(\"  5. Build a web interface with Flask/Streamlit\")\n",
        "\n",
        "    print(\"\\n\" + \"█\"*70)\n",
        "\n",
        "    return model, recommender, data, test_rmse\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN ENTRY POINT\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"\n",
        "    Run the complete pipeline\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\"\"\n",
        "    ╔════════════════════════════════════════════════════════════════╗\n",
        "    ║  MOVIELENS HYBRID RECOMMENDER - GITHUB CODESPACES READY        ║\n",
        "    ╠════════════════════════════════════════════════════════════════╣\n",
        "    ║                                                                ║\n",
        "    ║  This script will:                                             ║\n",
        "    ║  1. Auto-download MovieLens Latest-Small dataset (100K)        ║\n",
        "    ║  2. Load and prepare data with perfect joins (NO mismatch!)    ║\n",
        "    ║  3. Train hybrid collaborative + content-based model           ║\n",
        "    ║  4. Evaluate performance (expected RMSE: 0.85-0.88)            ║\n",
        "    ║  5. Generate sample recommendations                            ║\n",
        "    ║  6. Create visualization plots                                 ║\n",
        "    ║                                                                ║\n",
        "    ║  No manual setup required - just run!                          ║\n",
        "    ║                                                                ║\n",
        "    ╚════════════════════════════════════════════════════════════════╝\n",
        "    \"\"\")\n",
        "\n",
        "    try:\n",
        "        # Run the complete pipeline\n",
        "        model, recommender, data, test_rmse = main()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"SUCCESS! Model and recommender ready to use.\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Save model and recommender for later use\n",
        "        print(\"\\n💾 Objects available in memory:\")\n",
        "        print(\"  • model: HybridMatrixFactorization instance\")\n",
        "        print(\"  • recommender: MovieRecommender interface\")\n",
        "        print(\"  • data: Complete dataset (pandas DataFrame)\")\n",
        "        print(\"  • test_rmse: Final test performance\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"❌ ERROR OCCURRED\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Error: {str(e)}\")\n",
        "\n",
        "        import traceback\n",
        "        print(\"\\nFull traceback:\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "        print(\"\\n🔧 TROUBLESHOOTING:\")\n",
        "        print(\"  1. Ensure you have internet connection (for download)\")\n",
        "        print(\"  2. Check available disk space (need ~50MB)\")\n",
        "        print(\"  3. Verify Python packages installed:\")\n",
        "        print(\"     pip install pandas numpy scikit-learn matplotlib seaborn\")\n",
        "        print(\"  4. If download fails, manually download from:\")\n",
        "        print(\"     https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(model, '/content/hybrid_recommender.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXD_-FszW6AT",
        "outputId": "25cd73b8-b328-41c5-fb5b-da57a01eb670"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/hybrid_recommender.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = joblib.load('/content/hybrid_recommender.pkl')\n"
      ],
      "metadata": {
        "id": "AU5SAed6qPZ0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile('/content/hybrid_recommender.zip', 'w') as zipf:\n",
        "    zipf.write('/content/hybrid_recommender.pkl', arcname='hybrid_recommender.pkl')"
      ],
      "metadata": {
        "id": "Fn8FjSXlqRvf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DC8WKdbuqidC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}